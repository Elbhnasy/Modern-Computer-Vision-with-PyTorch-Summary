{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ANN: الجوريزم بيحاكي دماغ الانسان بيتعلم من البيانات، زي اللي بيتعلمها الدماغ البشري. بتدريب الشبكة على كمية كبيرة من البيانات، تقدر تتعلم حل مشاكل مختلفة."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing AI and traditional machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning about the artificial neural network building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing feedforward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the hidden layer unit values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the output layer values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating loss values\n",
    "* Categorical variable prediction\n",
    "* Continuous variable prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward propagation in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A high-level strategy of coding feedforward propagation is:\n",
    "\n",
    "    1. Perform a sum product at each neuron.\n",
    "    2. Compute activation.\n",
    "    3. Repeat the first two steps at each neuron until the output layer.\n",
    "    4. Compute the loss by comparing the prediction with the actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(dot_value):\n",
    "    activate_value = 1 / (1 + np.exp(-dot_value)) \n",
    "    return activate_value  \n",
    "\n",
    "def mean_squared_error(pred_value, true_value):\n",
    "    mse = np.mean(np.square(pred_value - true_value))\n",
    "    return mse\n",
    "\n",
    "def feed_forward(inputs,outputs, weights):\n",
    "    # First hidden layer\n",
    "    first_hidden_layer = sigmoid(np.dot(inputs, weights[0]) + weights[1])\n",
    "    \n",
    "    # Second hidden layer and output layer\n",
    "    output_layer = sigmoid(np.dot(first_hidden_layer, weights[2]) + weights[3])\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mean_squared_error_value = mean_squared_error(output_layer,outputs)\n",
    "    return mean_squared_error_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(dot_value):\n",
    "    activate_value = 1 / (1 + np.exp(-dot_value)) \n",
    "    return activate_value\n",
    "\n",
    "def tanh(dot_value):\n",
    "    activate_value = (np.exp(dot_value)-np.exp(-dot_value))/(np.exp(dot_value)+np.exp(-dot_value))\n",
    "    return activate_value\n",
    "\n",
    "def relu(dot_value):\n",
    "    activate_value =np.where(dot_value>0,dot_value,0)\n",
    "    return activate_value\n",
    "\n",
    "def linear(dot_value):\n",
    "    return dot_value\n",
    "\n",
    "def softmax(dot_value):\n",
    "    activate_value = np.exp(dot_value)/np.sum(np.exp(dot_value))\n",
    "    return activate_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(pred_value, true_value):\n",
    "    mse = np.mean(np.square(pred_value - true_value))\n",
    "    return mse\n",
    "\n",
    "def mean_absolute_error(pred_value, true_value):\n",
    "    mae = np.mean(np.abs(pred_value - true_value))\n",
    "    return mae\n",
    "\n",
    "def binary_cross_entropy(pred_value, true_value):\n",
    "    bce = -np.mean(true_value*np.log(pred_value)+ (1-true_value)*np.log(1-pred_value))\n",
    "    return bce\n",
    "\n",
    "def categorical_cross_entropy(pred_value, true_value):\n",
    "    cce = -np.mean(np.log(pred_value[np.arange(len(true_value)),true_value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def update_weights(inputs, outputs, weights, learning_rate):\n",
    "    \n",
    "    # Deep copy of the original weights\n",
    "    original_weights = deepcopy(weights)\n",
    "\n",
    "    # Create temporary weights for updates\n",
    "    temp_weights = deepcopy(weights)\n",
    "\n",
    "    # Create a copy to store the updated weights\n",
    "    updated_weights = deepcopy(weights)\n",
    "\n",
    "    # Calculate the original loss\n",
    "    original_loss = feed_forward(inputs, outputs, original_weights)\n",
    "\n",
    "    # Loop through layers and weights\n",
    "    for i, layer in enumerate(original_weights):\n",
    "        for index, weight in np.ndenumerate(layer):\n",
    "            \n",
    "            # Create a deep copy of the weights for this iteration\n",
    "            temp_weights = deepcopy(weights)\n",
    "            \n",
    "            # Add a small perturbation to the weight\n",
    "            temp_weights[i][index] += 0.0001\n",
    "\n",
    "            # Calculate the loss with the perturbed weight\n",
    "            _loss_plus = feed_forward(inputs, outputs, temp_weights)\n",
    "\n",
    "            # Calculate the gradient\n",
    "            grad = (_loss_plus - original_loss) / 0.0001\n",
    "\n",
    "            # Update the weight using gradient descent\n",
    "            updated_weights[i][index] -= grad * learning_rate\n",
    "\n",
    "    # Return the updated weights and the original loss\n",
    "    return updated_weights, original_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "y = np.array([0])\n",
    "\n",
    "weights = [\n",
    "    np.array([[0.8, 0.4, 0.3], [0.2, 0.9, 0.5]]),\n",
    "    np.array([0.1]),\n",
    "    np.array([0.3, 0.5, 0.9]),\n",
    "    np.array([0.3])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0.79999867, 0.39999812, 0.29999563],\n",
       "         [0.19999867, 0.89999812, 0.49999563]]),\n",
       "  array([0.09999242]),\n",
       "  array([0.29998226, 0.49998103, 0.89998319]),\n",
       "  array([0.29997635])],\n",
       " 0.6842865333808263)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_weights(x,y,weights=weights,learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing backpropagation using the chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting feedforward propagation and backpropagation together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the impact of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
